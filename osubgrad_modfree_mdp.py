# -*- coding: utf-8 -*-
"""oProbing-MDP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xiU0ySvBp6S2dlj0BzHj9qb_MuR99Y4d
"""

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from torch.optim import Adam, RMSprop
from collections import deque
import operator
from functools import reduce
import numpy as np
from numpy import linalg as LA
from scipy.optimize import linprog
import math
import random
from collections import namedtuple
import seaborn as sns
sns.set()
import matplotlib.pyplot as plt
plt.switch_backend('agg')
from copy import deepcopy
import argparse
import pickle
import os
import time

"""# **HELPER FUNCTIONS: NUMPY**"""


def prod(iterable):
    return reduce(operator.mul, iterable, 1)


def read_file(filename):
    """
    Args:
        - filename [str]                      : Path to the file to read
                in the .dpomdp format defined at masplan.org
    Returns:
        - raw_data [dict of str:list of str] : Dictionnary indexed by tokens
                containing list of parts of the file prefixed by this token
    """
    with open(filename, 'r') as f:
        raw_file = '\n'.join(l.strip() for l in f if not l.startswith('#'))
    tokens = ["agents", "discount", "values", "states",
              "start", "start include", "start exclude",
              "actions", "observations", "T", "O", "R"]
    raw_data = {tok: [] for tok in tokens}
    splits = []
    last = 0
    for tok in tokens:
        pos = raw_file.find(tok + ':', last)
        while pos > -1:
            splits.append((tok, pos))
            last = pos
            pos = raw_file.find(tok + ':', pos + len(tok))
    for (tok, pos), (_, nxt) in zip(splits[:-1], splits[1:]):
        raw_data[tok].append(raw_file[pos + len(tok) + 1:nxt])
    (tok, pos) = splits[-1]
    raw_data[tok].append(raw_file[pos + len(tok) + 1:])
    return raw_data


def read_line(raw_str):
    """
    Args:
        - raw_str [str]         : Line extracted from parsed file
    Returns:
        - fields  [list of str] : List of non-empty, stripped fields
          (delimited by ':')
    """
    return [field.strip() for field in raw_str.split(':') if field]


def read_field(raw_str):
    """
    Args:
        - raw_str [str]         : Field extracted from parsed file
    Returns:
        - values  [list of str] : List of stripped values
          (delimited by ' ') in the field
    """
    return [val.strip(" \n\"\'") for val in raw_str.split()]


def read_count_or_enum(raw_str, prefix):
    """
    Args:
        - raw_str [str]         : Field extracted from parsed file
        - prefix  [str]         : String to prepend to a counter
          to generate item names when only a count is read
    Returns:
        - data    [list of str] : List of items
        - count   [int]         : Number of items
    """
    try:
        count = int(raw_str)
        return ["{}{}".format(prefix, i) for i in range(count)]
    except ValueError:
        return read_field(raw_str)


def read_id_or_item(raw_str, items):
    """
    Args:
        - raw_str [str]         : Value extracted of a field
        - items   [list of str] : List of items to search in
    Returns:
        - id      [int]         : Index of the item corresponding to the value
    """
    try:
        return int(raw_str)
    except ValueError:
        return items.index(raw_str)


def match_item(raw_str, items):
    """
    Args:
        - raw_str [str]                 : Field extracted from a line
        - items   [list of list of str] : List of list of items to match from
    Yields:
        - id      [int]       : Index of a joint item matching with the value
    """
    if raw_str == "*":
        if len(items) > 1:
            for ji in match_item(" ".join("*" * len(items)), items):
                yield ji
        else:
            for i in range(len(items[0])):
                yield i
    else:
        if len(items) > 1:
            left, cur = raw_str.rsplit(maxsplit=1)
            for ji in match_item(left, items[:-1]):
                for i in match_item(cur, [items[-1]]):
                    yield len(items[-1]) * ji + i
        else:
            yield read_id_or_item(raw_str, items[0])


def read_items(raw_str, prefixes):
    """
    Args:
        - raw_str  [str]                 : Part of file with one field per line
        - prefixes [list of str]         : List of string to prepend to a
                counter to generate item names when only a count is read
    Returns:
        - items    [list of list of str] : List of list of items
    """
    lines = raw_str.splitlines()
    return [
        read_count_or_enum(line, prefix)
        for line, prefix in zip(lines[1:], prefixes)
    ]


def read_start(raw_dict, states):
    """
    Args:
        - raw_dict [dict of str:list of str] : Dictionnary indexed by tokens
                containing list of parts of the file prefixed by this token
        - states   [list of st: List of the states extracted from .dpomdp file
    Returns:
        - start    [np.array (|S|)] : Initial state probability
                   distribution
    """
    if raw_dict["start"]:
        lines = raw_dict["start"][0].splitlines()
        fields = read_line(lines[0])
        if len(fields) == 0:
            f = read_field(lines[1])
            if f[0] == "uniform":
                start = np.ones(len(states)) / len(states)
            else:
                start = np.array([float(p) for p in f])
        elif len(fields) == 1:
            start = np.zeros(len(states))
            start[read_id_or_item(fields[0], states)] = 1
    elif raw_dict["start include"]:
        included = read_field(raw_dict["start include"][0])
        start = np.zeros(len(states))
        for s_id in map(lambda s: read_id_or_item(s, states), included):
            start[s_id] = 1 / len(included)
    elif raw_dict["start exclude"]:
        excluded = read_field(raw_dict["start exclude"][0])
        start = np.ones(len(states)) / (len(states) - len(excluded))
        for s_id in map(lambda s: read_id_or_item(s, states), excluded):
            start[s_id] = 0
    return start


def read_transition(raw_list, states, actions):
    n_ja = prod([len(a) for a in actions])
    transition_mat = np.zeros((n_ja, len(states), len(states)))
    for rule in raw_list:
        lines = rule.splitlines()
        fields = read_line(lines[0])
        if len(lines) > 1:
            f = read_field(lines[1])
        for ja in match_item(fields[0], actions):
            if len(fields) > 1:
                for s in match_item(fields[1], [states]):
                    if len(fields) > 2:
                        for nxt in match_item(fields[2], [states]):
                            transition_mat[ja, s, nxt] = float(fields[3])
                    elif f[0] == "uniform":
                        transition_mat[ja, s, :] = 1 / dpomdp.n_states
                    elif f[0] == "identity":
                        transition_mat[ja, s, :] = 0
                        transition_mat[ja, s, s] = 1
                    else:
                        for nxt, p in enumerate(f):
                            transition_mat[ja, s, nxt] = float(p)
            elif f[0] == "uniform":
                transition_mat[ja, :, :] = 1 / len(states)
            elif f[0] == "identity":
                transition_mat[ja, :, :] = np.eye(len(states))
            else:
                for s in range(len(states)):
                    for nxt, p in enumerate(read_field(lines[s + 1])):
                        transition_mat[ja, s, nxt] = float(p)
    return transition_mat


def read_observation(raw_list, states, actions, observations):
    n_ja = prod([len(a) for a in actions])
    n_jz = prod([len(z) for z in observations])
    observation_mat = np.zeros((n_ja, len(states), n_jz))
    for rule in raw_list:
        lines = rule.splitlines()
        fields = read_line(lines[0])
        if len(lines) > 1:
            f = read_field(lines[1])
        for ja in match_item(fields[0], actions):
            if len(fields) > 1:
                for nxt in match_item(fields[1], [states]):
                    if len(fields) > 2:
                        for jz in match_item(fields[2], observations):
                            observation_mat[ja, nxt, jz] = float(fields[3])
                    elif f[0] == "uniform":
                        observation_mat[ja, nxt, :] = 1 / n_jz
                    else:
                        for jz, p in enumerate(f):
                            observation_mat[ja, nxt, jz] = float(p)
            elif f[0] == "uniform":
                observation_mat[ja, :, :] = 1 / n_jz
            else:
                for nxt in range(dpomdp.n_states):
                    for jz, p in enumerate(_read_field(lines[s + 1])):
                        observation_mat[ja, nxt, jz] = float(p)
    return observation_mat


def read_reward(raw_list, states, actions, observations):
    n_ja = prod([len(a) for a in actions])
    n_jz = prod([len(z) for z in observations])
    reward_mat = np.zeros((n_ja, len(states), len(states), n_jz))
    for rule in raw_list:
        lines = rule.splitlines()
        fields = read_line(lines[0])
        for ja in match_item(fields[0], actions):
            for s in match_item(fields[1], [states]):
                if len(fields) > 2:
                    for nxt in match_item(fields[2], [states]):
                        if len(fields) > 3:
                            for jz in match_item(fields[3], observations):
                                reward_mat[ja, s, nxt, jz] = float(fields[4])
                        else:
                            for jz, r in enumerate(read_field(lines[1])):
                                reward_mat[ja, s, nxt, jz] = float(r)
                else:
                    for nxt in range(len(states)):
                        for jz, r in enumerate(read_field(lines[1 + nxt])):
                            reward_mat[ja, s, nxt, jz] = float(r)
    return reward_mat

"""# **DEC-MDPs: NUMPY**"""


class DecPOMDP():
    """
    Class representing an instance of Decentralized Partially Observable Markov Decision Process
    and providing a 'reset()', 'execute()' interface to sample minibatch of trajectories.
    (Public) Attributes:
        - agents                [list of Agent] : List of agents (see DecPOMDP.Agent)
        - n_agents              [int]           : Number of agents (abbrev. |Ag|)
        - discount              [float]         : Discount factor applied to future rewards
        - has_reward / has_cost [bool]          : Type of optimization described (max reward or min cost)
        - states                [list of str]   : List of unobservable/internal states
        - n_states              [int]           : Number of internal states (abbrev. |S|)
        - n_joint_actions       [int]           : Number of joint actions (abbrev. |JA|)
        - n_joint_observations  [int]           : Number of joint observations (abbrev. |JZ|)
        - start                 [np.ndarray (|S|)] :
                Initial state probability distribution
        - transition_mat        [np.ndarray (|JA| x |S| x |S|)] :
                Probability of transitionning from state s (dim 1) to state s' (dim 2)
                when executing joint action ja (dim 0)
        - observation_mat       [np.ndarray (|JA| x |S| x |JZ|)] :
                Probability of observing jz (dim 2) after executing joint action ja (dim 0)
                and landing in state s' (dim 1)
        - reward_mat            [np.ndarray (|JA| x |S| x |S| x |JZ|)] :
                Reward jointly obtained by the agents when experiencing
                transition from state s (dim 1) to state s' (dim 2)
                executing joint action ja (dim 0) and observing jz (dim 3)
    """
    class Agent:
        """
        Internal class storing all parameters for a single agent
        (Public) Attributes:
            - name           [str]
            - actions        [list of str]
            - n_actions      [int]
            - observations   [list of str]
            - n_observations [int]
        """

        def __init__(self, name, actions=[], observations=[]):
            self.name = name
            self.actions = actions
            self.n_actions = len(actions)
            self.observations = observations
            self.n_observations = len(observations)

    @staticmethod
    def parse(filename):
        """
        Generate DecPOMDP instance from text file in the .dpomdp format.
        """
        raw_data = read_file(filename)
        ag_names = read_count_or_enum(raw_data["agents"][0], "ag")
        ag_actions = read_items(raw_data["actions"][0], [
                                name + "_a" for name in ag_names])
        ag_observations = read_items(raw_data["observations"][0], [
                                     name + "_z" for name in ag_names])
        states = read_count_or_enum(raw_data["states"][0], "s")
        return DecPOMDP(
            agents=[DecPOMDP.Agent(name, a, z) for name, a, z in zip(
                ag_names, ag_actions, ag_observations)],
            discount=float(raw_data["discount"][0]),
            val_type=read_field(raw_data["values"][0])[0],
            states=states,
            start=read_start(raw_data, states),
            transition_mat=read_transition(raw_data["T"], states, ag_actions),
            observation_mat=read_observation(
                raw_data["O"], states, ag_actions, ag_observations),
            reward_mat=read_reward(
                raw_data["R"], states, ag_actions, ag_observations)
        )

    @staticmethod
    def load(filename):
        """
        Load a DecPOMDP instance from a .pyth file previously saved.
        """
        f = open(filename, 'rb')
        tmp_dict = pickle.load(f)
        f.close()

        mdp = DecPOMDP(states=[1])

        mdp.__dict__.update(tmp_dict)

        return mdp

    def save(self, filename):
        """
        Save as a .pyth binary file for faster future instanciations.
        """
        f = open(filename, 'wb')
        pickle.dump(self.__dict__, f, 2)
        f.close()

    def get_num_states(self):
        return self.n_states

    def get_discount(self):
        return self.discount

    def get_num_actions(self):
        return self.n_joint_actions

    def get_start(self):
        return self.start

    def get_reward(self):
        return self.reward_action_mat

    def get_transition_probability(self):
        return self.transition_mat

    def get_state_possible_actions(self):
        tmat = self.transition_mat.sum(axis=2)
        actions, states = np.nonzero(tmat)
        result = {s: [] for s in range(self.n_states)}
        for i in range(len(actions)):
            s = states[i]
            a = actions[i]
            result[s].append(a)

        for s in range(self.n_states):
            result[s].sort()

        return result

    def __init__(self, agents=[], discount=1.0, val_type="reward",
                 states=[], start=None, transition_mat=None,
                 observation_mat=None, reward_mat=None):
        """
        Constructor.
        """
        super().__init__()

        self.agents = agents
        self.n_agents = len(agents)
        self.discount = discount
        self.has_reward = (val_type == "reward")
        self.has_cost = (val_type == "cost")
        self.states = states
        self.n_states = len(states)
        self.n_joint_actions = prod([ag.n_actions for ag in self.agents])
        self.n_joint_observations = prod(
            [ag.n_observations for ag in self.agents])

        self.start = np.ones(self.n_states) / \
            self.n_states if start is None else start
        self.transition_mat = np.ones(
            (self.n_joint_actions, self.n_states, self.n_states)
        ) / self.n_states if transition_mat is None else transition_mat
        self.observation_mat = np.ones(
            (self.n_joint_actions, self.n_states, self.n_joint_observations)
        ) / self.n_joint_observations if observation_mat is None else observation_mat
        self.reward_mat = np.ones(
            (self.n_joint_actions, self.n_states,
             self.n_states, self.n_joint_observations)
        ) * -1.0 if reward_mat is None else reward_mat

        m, M = self.reward_mat.min(), self.reward_mat.max()
        scaled = (2 * self.reward_mat - (M + m)) / (M - m)
        self.scaled_reward = scaled

        # [torch.FloatTensor (|JA| x |S|)]
        reward_action_mat = (
            (self.reward_mat * np.expand_dims(self.observation_mat, 1)).sum(axis=3) *
            self.transition_mat
        ).sum(axis=2)
        scaled_reward_action = (
            (self.scaled_reward * np.expand_dims(self.observation_mat, 1)).sum(axis=3) *
            self.transition_mat
        ).sum(axis=2)

        self.reward_action_mat = reward_action_mat
        self.scaled_reward_action = scaled_reward_action

        self.scaled = False

    def __str__(self):
        """
        Format readable string for this DecPOMDP instance.
        """
        return "agents: " + ' '.join([ag.name for ag in self.agents]) + '\n' \
            + "discount: {}\n".format(self.discount) \
            + "values: {}\n".format("reward" if self.has_reward else "cost") \
            + "states: " + ' '.join(self.states) + '\n' \
            + "start:\n" \
            + ' '.join(str(p) for p in self.start) + '\n' \
            + "actions:\n" \
            + '\n'.join(' '.join(ag.actions) for ag in self.agents) + '\n' \
            + "observations:\n" \
            + '\n'.join(' '.join(ag.observations) for ag in self.agents) + '\n' \
            + "T: Tensor of size {}\n".format("x".join(str(d) for d in self.transition_mat.shape)) \
            + "O: Tensor of size {}\n".format("x".join(str(d) for d in self.observation_mat.shape)) \
            + "R: Tensor of size {}\n".format("x".join(str(d)
                                                       for d in self.reward_mat.shape))

    def get_formated_joint_action(self, joint_action_id):
        """
        Format a readable string for the joint action corresponding to a given index.
        """
        indiv_ids = self.get_indiv_action_ids(joint_action_id)
        return ", ".join(ag.actions[a] for ag, a in zip(self.agents, indiv_ids))

    def get_formated_joint_observation(self, joint_observation_id):
        """
        Format a readable string for the joint observation corresponding to a given index.
        """
        indiv_ids = self.get_indiv_observation_ids(joint_observation_id)
        return ", ".join(ag.observations[z] for ag, z in zip(self.agents, indiv_ids))

    def get_joint_action_id(self, action_ids):
        """
        Compute the unique joint action index corresponding to the given individual actions' indices.
        Args:
            - action_ids [list of torch.LongTensor N] : List of (batch of) individual action(s)
                        OR [list of int]
        Returns:
            - joint_id   [torch.LongTensor N]         : (Batch of) joint action(s)
                        OR [int]
        """
        joint_id = action_ids[0]
        for ag, a_id in zip(self.agents[1:], action_ids[1:]):
            joint_id = ag.n_actions * joint_id + a_id
        return joint_id

    def get_joint_observation_id(self, observation_ids):
        """
        Compute the unique joint observation index corresponding to the given individual observations' indices.
        Args:
            - observation_ids [list of torch.LongTensor N] : List of (batch of) individual observation(s)
                            OR [list of int]
        Returns:
            - joint_id        [torch.LongTensor N]         : (Batch of) joint observation(s)
                            OR [int]
        """
        joint_id = observation_ids[0]
        for ag, o_id in zip(self.agents[1:], observation_ids[1:]):
            joint_id = ag.n_observations * joint_id + o_id
        return joint_id

    def get_indiv_action_ids(self, joint_action_id):
        """
        Compute the individual actions' indices corresponding to the given joint action index.
        Args:
            - joint_action_id [torch.LongTensor N] : (Batch of) joint action(s)
                            OR [int]
        Returns:
            - indiv_ids       [list of torch.LongTensor N] : List of (batch of) individual action(s)
                            OR [list of int]
        """
        ja = joint_action_id
        indiv_ids = []
        for ag in reversed(self.agents):
            if isinstance(ja, np.ndarray):
                ja, a = np.floor_divide(
                    ja, ag.n_actions), np.remainder(ja, ag.n_actions)
            else:
                ja, a = divmod(ja, ag.n_actions)
            indiv_ids.append(a)
        indiv_ids.reverse()
        return indiv_ids

    def get_indiv_observation_ids(self, joint_observation_id):
        """
        Compute the individual observations' indices corresponding to the given joint observation index.
        Args:
            - joint_observation_id [torch.LongTensor N]         : (Batch of) joint observation(s)
                                OR [int]
        Returns:
            - indiv_ids            [list of torch.LongTensor N] : List of (batch of) individual observation(s)
                                OR [list of int]
        """
        jz = joint_observation_id
        indiv_ids = []
        for ag in reversed(self.agents):
            if isinstance(jz, np.ndarray):
                jz, z = np.floor_divide(jz, ag.n_observations), np.remainder(
                    jz, ag.n_observations)
            else:
                jz, z = divmod(jz, ag.n_observations)
            indiv_ids.append(z)
        indiv_ids.reverse()
        return indiv_ids

    def reset(self, batch_size=1):
        """
        Initialize internal state and belief.
        Args:
            - batch_size [int] : Size of batch
        """
        self.internal = np.array([
            np.argmax(np.random.multinomial(1, self.start))
            for i in range(batch_size)
        ])
        self.belief = np.concatenate(
            [np.expand_dims(self.start, 0)] * batch_size)

        if batch_size == 1:
            return self.internal[0]
        else:
            return self.internal

    def get_current_state(self):
        """
        Get current state index.
        Returns:
            - internal [LongTensor N x 1] : Batch of internal (unobservable) states.
        """
        return self.internal.reshape(list(self.internal.shape) + [1])

    def get_current_belief(self):
        """
        Get current belief per state.
        Returns:
            - belief [FloatTensor N x |S|] : Batch of states' belief
        """
        return self.belief

    def exec_one_step(self, state, action):

        nxt = self.transition_mat[action, state]
        nxt_batch = nxt.reshape((-1, self.n_states))
        if nxt_batch.shape[0] == 1:
            nxt = np.argmax(np.random.multinomial(1, nxt_batch[0]))
        else:
            nxt = np.array([
                np.argmax(np.random.multinomial(1, nxt_batch[i]))
                for i in range(nxt_batch.shape[0])
            ])

        return nxt

    def step_with_state(self, state, action):

        nxt = self.exec_one_step(state, action)
        rew = self.reward_action_mat[action, state]
        rew_batch = rew.reshape((-1, 1))
        if rew_batch.shape[0] == 1:
            rew = rew_batch[0, 0]

        return nxt, rew

    def step(self, action):

        if len(self.internal) == 1:
            state = self.internal[0]
        else:
            state = self.internal

        nxt, rew = self.step_with_state(state, action)

        if isinstance(nxt, (int, np.int64, np.int32,)):
            state = np.array([nxt])
        else:
            state = nxt

        self.internal = state

        return nxt, rew

    def execute(self, joint_action_id):
        """
        Execute joint action, updating internal state and belief, and returning joint observation and reward.
        Args:
            - joint_action_id [LongTensor  N x 1] : Batch of join actions
        Returns:
            - jz              [LongTensor  N x 1] : Batch of joint observations
            - r               [FloatTensor N x 1] : Batch of rewards
        """
        ja = np.squeeze(joint_action_id, -1)

        nxt = self.transition_mat[ja, self.internal]
        nxt = np.array([
            np.argmax(np.random.multinomial(1, nxt[i]))
            for i in range(nxt.shape[0])
        ])
        jz = self.observation_mat[ja, nxt]
        jz = np.array([
            np.argmax(np.random.multinomial(1, jz[i]))
            for i in range(jz.shape[0])
        ])
        r = self.reward_mat[ja, self.internal, nxt, jz]
        r_scaled = self.scaled_reward[ja, self.internal, nxt, jz]

        self.internal = nxt
        self.belief = np.transpose(self.observation_mat, (0, 2, 1))[ja, jz, :] \
            * (self.transition_mat[ja, :, :] * self.belief.reshape(list(self.belief.shape) + [1])).sum(axis=1)
        self.belief = self.belief / self.belief.sum(axis=1, keepdims=True)

        return jz.reshape(list(jz.shape) + [1]), r.reshape(list(r.shape) + [1]), r_scaled.reshape(list(r_scaled.shape) + [1])

    def use_scaled_reward(self, t=True):
        """
        Normalize reward in [-1,1] or restore original values.
        """
        if self.scaled != t:
            self.reward_mat, self.scaled_reward = self.scaled_reward, self.reward_mat
            self.reward_action_mat, self.scaled_reward_action = self.scaled_reward_action, self.reward_action_mat
            self.scaled = t


"""#**REPLAY MEMORY**"""

Experience = namedtuple(
    "Experience",
    ("states", "actions", "rewards", "next_states", "dones")
)


class ReplayMemory(object):
    """
    Replay memory buffer
    """

    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = []
        self.position = 0

    def _push_one(self, state, action, reward, next_state=None, done=None):
        if len(self.memory) < self.capacity:
            self.memory.append(None)
        self.memory[self.position] = Experience(
            state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity

    def push(self, states, actions, rewards, next_states=None, dones=None):
        if isinstance(states, list):
            if next_states is not None and len(next_states) > 0:
                for s, a, r, n_s, d in zip(states, actions, rewards, next_states, dones):
                    self._push_one(s, a, r, n_s, d)
            else:
                for s, a, r in zip(states, actions, rewards):
                    self._push_one(s, a, r)
        else:
            self._push_one(states, actions, rewards, next_states, dones)

    def sample(self, batch_size):
        if batch_size > len(self.memory):
            batch_size = len(self.memory)
        transitions = random.sample(self.memory, batch_size)
        batch = Experience(*zip(*transitions))
        states = np.array(batch.states)
        actions = np.array(batch.actions)
        rewards = np.array(batch.rewards)
        next_states = np.array(batch.next_states)
        dones = np.array(batch.dones)
        batch = Experience(states, actions, rewards, next_states, dones)
        return batch

    def clear(self):
        self.memory.clear()
        self.position = 0

    def __len__(self):
        return len(self.memory)


"""#**O-SubgradModFree**"""


class OSubgradModFree():

    def __init__(
            self, penalty, eps_end, eps_start, eps_decay,
            rate_end, rate_start, rate_decay, episodes, batch_size,
            capacity=1000, backup=False):

        self.penalty = penalty
        self.episodes = episodes

        # //<! initialize learning-rate parameters:
        self.rate_end = rate_end
        self.rate_start = rate_start
        self.rate_decay = rate_decay

        #//<! initialize epsilon-exploration parameters:
        self.eps_end = eps_end
        self.eps_start = eps_start
        self.eps_decay = eps_decay

        #//<! initialize the backup
        self.backup = backup

        #//<! initialize the batch-size
        self.batch_size = batch_size

        self.trial = 0

        self.replayMem = ReplayMemory(capacity)
        # self.start_states = deque(capacity)
        self.start_states_dict = dict()
        self.num_reset = 0
        self.n_episodes = 0
        self.n_steps = 0

    def clear_memory(self):
        self.replayMem.clear()
        # self.start_states.clear()
        self.start_states_dict.clear()
        self.num_reset = 0
        self.n_steps = 0
        self.n_episodes = 0

    def do_reset_time_and_state(self):

        self.t = 0
        self.current_state = self.model.reset()

        if self.current_state not in self.start_states_dict:
            self.start_states_dict[self.current_state] = 0

        self.start_states_dict[self.current_state] += 1
        # self.start_states.append(self.current_state)
        self.num_reset += 1

    # take one step
    def _take_one_step(self):
        if (self.t_max is not None) and (self.t > self.t_max):
            self.do_reset_time_and_state()
        state = self.current_state
        action = self.exploration_action(state)
        next_state, reward = self.model.step(action)
        self.t += 1
        done = False
        if self.t > self.t_max:
            done = True
        if done:
            self.do_reset_time_and_state()
            self.n_episodes += 1
            self.episode_done = True
        else:
            self.current_state = next_state
            self.episode_done = False
        self.replayMem.push(state, action, reward, next_state, done)

    def exploration_action(self, state):
        sample = random.random()
        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \
            np.exp(-1. * self.n_steps / self.eps_decay)
        self.n_steps += 1
        if sample > eps_threshold:
            data = self.current_policy[:, state]
            a = np.argmax(data, axis=0)
        else:
            a = random.randrange(self.model.get_num_actions())

        return a

    def do_initialize(self):

        self.clear_memory()
        self.do_reset_time_and_state()
        self.discount = self.model.get_discount()

        self.v_mlagrange = np.zeros(
            self.model.get_num_states() + 1
        )

        self.v_occupancy = np.random.rand(
            self.model.get_num_actions(), self.model.get_num_states()
        )
        self.v_occupancy = self.v_occupancy / self.v_occupancy.sum()

        temp = self.v_occupancy.sum(axis=0, keepdims=True)
        temp[temp == 0] = 1.0
        self.current_policy = self.v_occupancy / temp

    def do_interaction(self):
        self._take_one_step()

    def do_train_one_sample(self):
        if len(self.replayMem) < self.batch_size:
            return None, None

        amplitude = LA.norm(self.v_occupancy) + LA.norm(self.v_mlagrange)

        # if len(self.start_states) < self.batch_size:
        #    return
        batch1 = self.replayMem.sample(self.batch_size)
        batch2 = self.replayMem.sample(self.batch_size)
        batch_init_states_1 = np.array(
            [self.start_states_dict[s] / self.num_reset
             if s in self.start_states_dict else 0.0
             for s in batch1.next_states
             ]
        )
        batch_init_states_2 = np.array(
            [self.start_states_dict[s] / self.num_reset
             if s in self.start_states_dict else 0.0
             for s in batch2.next_states
             ]
        )

        second = (
            ((batch1.states == batch1.next_states) - self.discount
             ) * self.v_occupancy[batch1.actions, batch1.states]
            - (1 - self.discount) * batch_init_states_1
        )

        first_A = (
            ((batch2.states == batch2.next_states) - self.discount
             ) - (1 - self.discount) * batch_init_states_2
        )

        c1 = self.penalty * (batch2.next_states ==
                             batch1.next_states) * first_A
        c2 = self.v_mlagrange[batch1.next_states] * second
        c3 = batch1.rewards

        c = c1 + c2 + c3

        delta_occupancy = np.zeros(
            (self.model.get_num_actions(), self.model.get_num_states())
        )
        n_occupancy = np.zeros(
            (self.model.get_num_actions(), self.model.get_num_states())
        )
        for i, (s, a) in enumerate(zip(batch1.states, batch1.actions)):
            delta_occupancy[a, s] += c[i]
            n_occupancy[a, s] += 1
        n_occupancy[n_occupancy == 0] = 1
        delta_occupancy = delta_occupancy / n_occupancy

        self.v_occupancy = self.v_occupancy + \
            (self.learning_rate / amplitude) * delta_occupancy
        self.v_occupancy = np.maximum(self.v_occupancy, 0.0)

        delta_lagrange = np.zeros((self.model.get_num_states() + 1,))
        n_langrange = np.zeros((self.model.get_num_states() + 1,))
        for i, s in enumerate(batch1.next_states):
            delta_lagrange[s] += second[i]
            n_langrange[s] += 1
        n_langrange[n_langrange == 0] = 1
        delta_lagrange[self.model.get_num_states(
        )] = self.v_occupancy.sum() - 1.0
        delta_lagrange = delta_lagrange / n_langrange

        self.v_mlagrange = self.v_mlagrange - \
            (self.learning_rate / amplitude) * delta_lagrange

        reward = (self.v_occupancy * self.model.get_reward()).sum()
        error = LA.norm(second)

        temp = self.v_occupancy.sum(axis=0, keepdims=True)
        temp[temp == 0] = 1.0
        self.current_policy = self.v_occupancy / temp

        #print("solution: ", self.v_occupancy)

        return reward, error

    def do_train_one_sample_2(self):
        if len(self.replayMem) < self.batch_size:
            return None, None

        amplitude = LA.norm(self.v_occupancy) + LA.norm(self.v_mlagrange)

        # if len(self.start_states) < self.batch_size:
        #    return
        batch1 = self.replayMem.sample(self.batch_size)
        batch2 = self.replayMem.sample(self.batch_size)
        batch_init_states_1 = np.array(
            [self.start_states_dict[s] / self.num_reset
             if s in self.start_states_dict else 0.0
             for s in batch1.next_states
             ]
        )
        batch_init_states_2 = np.array(
            [self.start_states_dict[s] / self.num_reset
             if s in self.start_states_dict else 0.0
             for s in batch2.next_states
             ]
        )

        use_second_batch = False
        if use_second_batch:
            batch_second = batch2
            batch_second_init = batch_init_states_2
        else:
            batch_second = batch1
            batch_second_init = batch_init_states_1

        second = (
            (1 - self.discount) * batch_second_init -
            (self.v_occupancy[:, batch_second.next_states].sum(axis=0)
                - self.discount *
             self.v_occupancy[batch_second.actions, batch_second.states]
             )
        )

        delta_second = np.zeros((self.model.get_num_states() + 1,))
        n_delta_second = np.zeros((self.model.get_num_states() + 1,))
        for i, s in enumerate(batch_second.next_states):
            delta_second[s] += second[i]
            n_delta_second[s] += 1
        n_delta_second[n_delta_second == 0] = 1
        delta_second = delta_second / n_delta_second
        delta_second[-1] = 1 - self.v_occupancy.sum()

        c3 = self.penalty * (
            delta_second[batch1.states] -
            self.discount *
            delta_second[batch1.next_states] + delta_second[-1]
        )
        c2 = self.v_mlagrange[batch1.states] - \
            self.discount * \
            self.v_mlagrange[batch1.next_states] + self.v_mlagrange[-1]
        c1 = batch1.rewards

        c = c1 - c2 + c3

        delta_occupancy = np.zeros(
            (self.model.get_num_actions(), self.model.get_num_states())
        )
        n_occupancy = np.zeros(
            (self.model.get_num_actions(), self.model.get_num_states())
        )
        for i, (s, a) in enumerate(zip(batch1.states, batch1.actions)):
            delta_occupancy[a, s] += c[i]
            n_occupancy[a, s] += 1
        n_occupancy[n_occupancy == 0] = 1
        delta_occupancy = delta_occupancy / n_occupancy

        self.v_occupancy = self.v_occupancy + \
            (self.learning_rate / amplitude) * delta_occupancy
        self.v_occupancy = np.maximum(self.v_occupancy, 0.0)

        #self.v_occupancy /= self.v_occupancy.sum()

        self.v_mlagrange = self.v_mlagrange - \
            (self.learning_rate / amplitude) * delta_second

        reward = (self.v_occupancy * self.model.get_reward()).sum()
        error = LA.norm(delta_second)

        temp = self.v_occupancy.sum(axis=0, keepdims=True)
        temp[temp == 0] = 1.0
        self.current_policy = self.v_occupancy / temp

        #print("solution: ", self.v_occupancy)

        return reward, error

    def do_train_one_sample_3(self):
        if len(self.replayMem) < self.batch_size:
            return None, None

        amplitude = LA.norm(self.v_occupancy) + LA.norm(self.v_mlagrange)

        # if len(self.start_states) < self.batch_size:
        #    return
        batch1 = self.replayMem.sample(self.batch_size)
        batch2 = self.replayMem.sample(self.batch_size)
        batch_init_states_1 = np.array(
            [self.start_states_dict[s] / self.num_reset
             if s in self.start_states_dict else 0.0
             for s in batch1.next_states
             ]
        )
        batch_init_states_2 = np.array(
            [self.start_states_dict[s] / self.num_reset
             if s in self.start_states_dict else 0.0
             for s in batch2.next_states
             ]
        )

        use_second_batch = False
        if use_second_batch:
            batch_second = batch2
            batch_second_init = batch_init_states_2
        else:
            batch_second = batch1
            batch_second_init = batch_init_states_1

        second = (
            (1 - self.discount) * batch_second_init -
            ((batch_second.states == batch_second.next_states)
                - self.discount *
                self.v_occupancy[batch_second.actions, batch_second.states]
             )
        )

        delta_second = np.zeros((self.model.get_num_states() + 1,))
        n_delta_second = np.zeros((self.model.get_num_states() + 1,))
        for i, s in enumerate(batch_second.next_states):
            delta_second[s] += second[i]
            n_delta_second[s] += 1
        n_delta_second[n_delta_second == 0] = 1
        delta_second = delta_second / n_delta_second
        delta_second[-1] = 1 - self.v_occupancy.sum()

        c3 = self.penalty * (
            delta_second[batch1.states] -
            self.discount *
            delta_second[batch1.next_states] + delta_second[-1]
        )
        c2 = self.v_mlagrange[batch1.states] - \
            self.discount * \
            self.v_mlagrange[batch1.next_states] + self.v_mlagrange[-1]
        c1 = batch1.rewards

        c = c1 - c2 + c3

        delta_occupancy = np.zeros(
            (self.model.get_num_actions(), self.model.get_num_states())
        )
        n_occupancy = np.zeros(
            (self.model.get_num_actions(), self.model.get_num_states())
        )
        for i, (s, a) in enumerate(zip(batch1.states, batch1.actions)):
            delta_occupancy[a, s] += c[i]
            n_occupancy[a, s] += 1
        n_occupancy[n_occupancy == 0] = 1
        delta_occupancy = delta_occupancy / n_occupancy

        self.v_occupancy = self.v_occupancy + \
            (self.learning_rate / amplitude) * delta_occupancy
        self.v_occupancy = np.maximum(self.v_occupancy, 0.0)

        #self.v_occupancy /= self.v_occupancy.sum()

        self.v_mlagrange = self.v_mlagrange - \
            (self.learning_rate / amplitude) * delta_second

        reward = (self.v_occupancy * self.model.get_reward()).sum()
        error = LA.norm(delta_second)

        temp = self.v_occupancy.sum(axis=0, keepdims=True)
        temp[temp == 0] = 1.0
        self.current_policy = self.v_occupancy / temp

        #print("solution: ", self.v_occupancy)

        return reward, error

    def do_train(self):
        # return self.do_train_one_sample()
        return self.do_train_one_sample_2()

    def do_update(self):

        amplitude = LA.norm(self.v_occupancy) + LA.norm(self.v_mlagrange)

        temp = self.freq_transition_mat.sum(axis=2, keepdims=True)
        temp[temp == 0] = 1.0
        freq_prob = self.freq_transition_mat / (
            temp
        ).reshape(
            list(self.freq_policy_mat.shape) + [1]
        )

        transition_mat = self.model.get_transition_probability()
        m_dynamics = np.zeros(
            (
                self.model.get_num_states() + 1,
                self.model.get_num_actions(),
                self.model.get_num_states()
            )
        )
        m_dynamics[self.model.get_num_states(), :, :] = 1.0
        m_dynamics[0:self.model.get_num_states(), :, :] = np.eye(
            self.model.get_num_states()
        ).reshape(
            self.model.get_num_states(),
            1,
            self.model.get_num_states()
        ) - self.model.get_discount() * np.transpose(transition_mat, (2, 0, 1))

        second = self.v_belief - (
            np.matmul(
                m_dynamics.reshape(self.model.get_num_states() + 1, -1),
                self.v_occupancy.reshape(-1)
            )
        )

        dynamics_transpose = np.transpose(
            m_dynamics.reshape(self.model.get_num_states() + 1, -1)
        )

        first = (self.v_rewards.reshape(-1) - (
            np.matmul(dynamics_transpose, self.v_mlagrange)
        ) + (
            self.penalty * np.matmul(dynamics_transpose, second)
        )).reshape(self.model.get_num_actions(), self.model.get_num_states())

        self.v_occupancy = self.v_occupancy + \
            (self.learning_rate / amplitude) * first
        self.v_occupancy = np.maximum(self.v_occupancy, 0.0)

        # self.v_occupancy = self.v_occupancy / self.v_occupancy.sum()
        # print('\nsum: ', self.v_occupancy.sum())

        self.v_mlagrange = self.v_mlagrange - \
            (self.learning_rate / amplitude) * second

        reward = (self.v_occupancy * self.v_rewards).sum()
        error = LA.norm(second)

        temp = self.v_occupancy.sum(axis=0, keepdims=True)
        temp[temp == 0] = 1.0
        self.current_policy = self.v_occupancy / temp

        #print("solution: ", self.v_occupancy)

        return reward, error

    def randomize(self, seed=None):

        np.random.seed(seed)
        random.seed(seed)

    def solve(self, env_id, world_env, horizon, learning_rate, tol, episode_before_train, num_trials=1, max_trial_time=0, seed=None, output_dir='.', save_freq=100, debugFlag=False):

        self.model = world_env
        self.learning_rate = learning_rate

        self.t_max = horizon
        self.tolerance = tol
        self.max_trial_time = max_trial_time

        allRewards = []
        allErrors = []
        allTimes = []
        allResults = []

        try:

            for trial in range(num_trials):

                self.trial = trial + 1

                print('\n')

                self.learning_rate = learning_rate

                self.randomize(
                    self.trial if seed is None else seed + self.trial)

                self.do_initialize()

                reward, error = 0.0, 0.0

                start = time.time()

                for episode in range(self.episodes):

                    self.episode = episode

                    self.learning_rate = self.rate_end + (
                        (self.rate_start - self.rate_end) *
                        np.exp(-self.episode / self.rate_decay)
                    )

                    self.do_interaction()

                    reward, error = 0.0, 0.0

                    if episode >= episode_before_train:

                        reward, error = self.do_train()

                        if reward is None:
                            if len(allRewards) > 0 and episode > 0:
                                reward, error = allRewards[
                                    -1][2], allErrors[-1][2]
                            else:
                                reward, error = 0.0, 0.0

                    elapsed = time.time() - start

                    if max_trial_time > 0 and elapsed > max_trial_time:
                        breakFlag = True
                    else:
                        breakFlag = False

                    if episode >= episode_before_train:
                        allRewards.append([trial, episode, reward, 3])
                        allErrors.append([trial, episode, error, 3])
                        allTimes.append([trial, episode, elapsed, 3])
                        allResults.append(
                            [trial, episode, np.absolute(self.v_occupancy).sum(), 3])

                    if debugFlag:
                        v = np.absolute(self.v_occupancy).sum()
                        pre = ["=" if (i < 64.0 * self.episode /
                                       self.episodes) else " " for i in range(64)]
                        pre = "".join(pre)

                        suf = " ] ({:.2f}%) -> loss = {:.3f}  reward = {:.3f}  step-size = {:.2f}%   norm1 = {:.3f} ".format(
                            100.0 * self.episode / self.episodes,
                            error, reward, 100.0 * self.learning_rate, v
                        )
                        print("Trial #{} Iterations #{} : [{} {}".format(
                            self.trial, self.episode, pre, suf), end='\r' if episode < self.episodes - 1 else '\n'
                        )
                        # print(np.absolute(self.v_occupancy).sum())

                    if episode > episode_before_train and ((episode % save_freq == 0) or (episode == self.episodes - 1) or breakFlag):

                        np.savetxt("%s/%s_osubmodfree_errors.txt" %
                                   (output_dir, env_id), allErrors)
                        np.savetxt("%s/%s_osubmodfree_rewards.txt" %
                                   (output_dir, env_id), allRewards)
                        np.savetxt("%s/%s_osubmodfree_times.txt" %
                                   (output_dir, env_id), allTimes)
                        np.savetxt("%s/%s_osubmodfree_occupancy.txt" %
                                   (output_dir, env_id), allResults, fmt='%.6f')

                    if breakFlag:
                        break

        except KeyboardInterrupt:
            print("\nW: interrupt received, stoppingâ€¦")
            np.savetxt("%s/%s_osubmodfree_errors.txt" %
                       (output_dir, env_id), allErrors)
            np.savetxt("%s/%s_osubmodfree_rewards.txt" %
                       (output_dir, env_id), allRewards)
            np.savetxt("%s/%s_osubmodfree_times.txt" %
                       (output_dir, env_id), allTimes)
            np.savetxt("%s/%s_osubmodfree_occupancy.txt" %
                       (output_dir, env_id), allResults, fmt='%.6f')

        fig = plt.figure()
        ax = fig.add_subplot(111)
        allErrors = np.array(allErrors)
        ax = sns.lineplot(x=allErrors[:, 1], y=allErrors[:, 2], ax=ax)
        ax.set_title("%s" % (env_id))
        ax.set_xlabel('Episode')
        ax.set_ylabel('Errors')
        ax.legend(['OSubgrad-ModFree'])
        plt.savefig("%s/%s_osubmodfree_errors.png" % (output_dir, env_id))

        fig = plt.figure()
        ax = fig.add_subplot(111)
        allRewards = np.array(allRewards)
        ax = sns.lineplot(x=allRewards[:, 1], y=allRewards[:, 2], ax=ax)
        ax.set_title("%s" % (env_id))
        ax.set_xlabel('Episode')
        ax.set_ylabel('Rewards')
        ax.legend(['OSubgrad-ModFree'])
        plt.savefig("%s/%s_osubmodfree_rewards.png" % (output_dir, env_id))

        fig = plt.figure()
        ax = fig.add_subplot(111)
        allTimes = np.array(allTimes)
        ax = sns.lineplot(x=allTimes[:, 1], y=allTimes[:, 2], ax=ax)
        ax.set_title("%s" % (env_id))
        ax.set_xlabel('Episode')
        ax.set_ylabel('Elapsed time')
        ax.legend(['OSubgrad-ModFree'])
        plt.savefig("%s/%s_osubmodfree_times.png" % (output_dir, env_id))

        fig = plt.figure()
        ax = fig.add_subplot(111)
        allResults = np.array(allResults)
        ax = sns.lineplot(x=allResults[:, 1], y=allResults[:, 2], ax=ax)
        ax.set_title("%s" % (env_id))
        ax.set_xlabel('Episode')
        ax.set_ylabel('Occupancy Frequency Norm 1')
        ax.legend(['OSubgrad-ModFree'])
        plt.savefig("%s/%s_osubmodfree_norm1.png" % (output_dir, env_id))

        return allRewards, allErrors


"""# **MAIN**"""


def main():

    parser = argparse.ArgumentParser(description='O-Probing')

    parser.add_argument('--env', type=str, default='recycling_robot.dpomdp',
                        help='filename of the env (default: recycling_robot.dpomdp)')
    parser.add_argument('--penalty', type=float, default=1,
                        help='penalty (default: 1)')
    parser.add_argument('--batchsize', type=int, default=32,
                        help='input batch size for training (default: 32)')
    parser.add_argument('--episodes', type=int, default=1000,
                        help='number of episodes to train (default: 1000)')
    parser.add_argument('--trials', type=int, default=10,
                        help='number of trials to performs (default: 10)')
    parser.add_argument('--seed', type=int, default=1234,
                        help='random seed (default: 1234)')
    parser.add_argument('--horizon', type=int, default=10,
                        help='horizon (default: 10)')
    parser.add_argument('--discount', type=float, default=0,
                        help='discount factor (default: 0)')
    parser.add_argument('--lr', type=float, default=1,
                        help='learning rate (default: 1)')
    parser.add_argument('--eps', type=float, default=1,
                        help='epsilon (default: 1)')
    parser.add_argument('--rs', type=float, default=1,
                        help='rate_start (default: 1)')
    parser.add_argument('--re', type=float, default=9.99999975e-05,
                        help='rate_end (default: 9.99999975e-05)')
    parser.add_argument('--rd', type=float, default=1000,
                        help='rate_decay (default: 1000)')
    parser.add_argument('--es', type=float, default=1,
                        help='eps_start (default: 1)')
    parser.add_argument('--ee', type=float, default=0.001,
                        help='eps_end (default: 0.001)')
    parser.add_argument('--ed', type=float, default=1000,
                        help='eps_decay (default: 1000)')
    parser.add_argument('--od', type=str, default='.',
                        help='output directory (default: .)')
    parser.add_argument('--normalize_reward',
                        help='normalize the reward', action='store_true')
    parser.add_argument('--save_freq', type=int, default=100,
                        help='frequency for saving the logs on disk (default: 100)')
    parser.add_argument('--tol', type=float, default=0.0,
                        help='tolerance for control process (default: 0.0)')
    parser.add_argument('--max_trial_time', type=int, default=0,
                        help='Maximum time in seconds for running a trial (default: 0)')
    parser.add_argument('--burned_episodes', type=int, default=0,
                        help='Number episodes to run before start training (default: 0)')
    parser.add_argument('--capacity', type=int, default=1e4,
                        help='size of the replay memory (default: 1e4)')

    args = parser.parse_args()

    env_id = args.env.split('/')[-1].split('.')[-2]
    print('env_id: ', env_id)
    env = DecPOMDP.parse(args.env)

    if args.discount > 0.0:
        env.discount = args.discount

    if args.normalize_reward:
        print('normalize rewards')
        env.use_scaled_reward(args.normalize_reward)

    probing = OSubgradModFree(

        args.penalty, args.ee, args.es, args.ed,
        args.re, args.rs, args.rd, args.episodes, args.batchsize,
        args.capacity, backup=True
    )

    allRewards, allErrors = probing.solve(
        env_id, env, args.horizon, args.lr, args.tol, args.burned_episodes,
        args.trials, args.max_trial_time, args.seed, output_dir=args.od,
        save_freq=args.save_freq, debugFlag=True
    )


if __name__ == "__main__":
    main()
